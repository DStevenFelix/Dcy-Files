# Reinforcement learning（强化学习）

##### 强化学习所解决的问题的特点：

1. 智能体和环境之间不断进行交互；
2. 搜索和试错；
3. 延迟奖励（当前所做的动作可能很多步之后才会产生相应的结果）。

##### 目标:

1. 获取更多的累积奖励；
2. 获得更可靠的估计。

##### 基于模型的强化学习:构建一个模型之后使用mdp算法迭代并将其应用到模型中

##### 无模型的强化学习:

样本以合适的频率出现。

##### 被动强化学习:

智能体遵循着 某种策略

##### 时序差分学习：从每次经验中学习

##### Q-Learning

Q-Learning 决策：在某种状态时选择能够带来更大的潜在奖励的动作a，之后转移到该种状态继续执行能够带来更大潜在奖励的动作。

Q-Learning更新【有点不理解】:通过之前的决策方法，我们在s1采取了a2，并到达s2，这时我们开始更新用于决策的Q表，接着我们并没有在实际中采取任何行为，而是再想象自己在s2上采取了每种行为，分别看看两种行为哪一个的Q值大，比如说Q(s2,a2)的值比Q(s2,a1)的大，所以我们把大的Q(s2,a2)乘上一个衰减值γ（比如是0.9）并加上到达s2时所获取的奖励R（这里我们还没有获取到棒棒糖，所以奖励为0）因为会获取实实在在的奖励R，我们将这个作为我现实中Q(s1,a2)的值，但是我们之前是根据Q表估计Q(s1,a2)的值。所以有了现实和估计值，我们就能更新Q(s1,a2)，根据估计与现实的差距，将这个差距乘以一个学习效率α累加上老的Q(s1,a2)的值变成新的值。但时刻记住，我们虽然用maxQ(s2)估算了一下s2的状态，但还没有在s2上做出任何的行为，s2的行为决策要等到更新完了以后再重新另外做。
Q估计：Q(s1,a2)

Q现实：R+γ*maxQ(s2)

计算出差距=现实-估计

newQ(s1,a2)=老Q(s1,a2)+a*差距





